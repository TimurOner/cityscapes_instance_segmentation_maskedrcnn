# -*- coding: utf-8 -*-
"""data_funs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOibGbD61EdyEF2e7AK7dyickJQtr9F7
"""





# Standard Library
import sys,re,os
import random
import cv2
import numpy as np
import pandas as pd
import gc
import copy
from collections import namedtuple, Counter, defaultdict
import warnings,json
from IPython.display import clear_output

# Data Science & Visualization
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.ticker as ticker

import seaborn as sns

# PyTorch and Torchvision
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, MultiStepLR
import torchvision
from PIL import Image
from torchvision import transforms
from torchvision import transforms as T
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

# Torchvision Detection Models and Utils (unused in the provided code)
from torchvision.models.detection import MaskRCNN
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.backbone_utils import BackboneWithFPN
import torchvision.models.detection.mask_rcnn as mask_rcnn
import torchvision.models.detection.faster_rcnn as faster_rcnn
from torchvision.models.detection import maskrcnn_resnet50_fpn
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor
from torchvision.models.detection.anchor_utils import AnchorGenerator
from torchvision.ops import box_convert,box_iou, generalized_box_iou
import torchvision.ops.boxes as box_ops

# Safetensor data loading tools
from safetensors.torch import load_file

#Torchmetrics
from torchmetrics.detection import MeanAveragePrecision
from torchmetrics.detection.mean_ap import MeanAveragePrecision

#Scipy and sklearn
from scipy.optimize import linear_sum_assignment
from sklearn.cluster import KMeans

#Mixed precision training
from torch.cuda.amp import autocast, GradScaler

# Progress bar
from tqdm import tqdm

# Set device


from pycocotools import mask as maskUtils
from pycocotools import mask as coco_mask

#Wandb
import wandb

#Optuna

import optuna
from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour
from optuna.importance import FanovaImportanceEvaluator


# =============================================================================
# LABELS AND ANNOTATION
# =============================================================================


Label = namedtuple('Label', [
    'name', 'id', 'trainId', 'category', 'categoryId', 'hasInstances', 'ignoreInEval', 'color'
])


cityscapes_labels = [
    Label('unlabeled',            0,   255, 'void',         0, False, True,  (0,   0,   0)),
    Label('ego vehicle',          1,   255, 'void',         0, False, True,  (0,   0,   0)),
    Label('rectification border', 2,   255, 'void',         0, False, True,  (0,   0,   0)),
    Label('out of roi',           3,   255, 'void',         0, False, True,  (0,   0,   0)),
    Label('static',               4,   255, 'void',         0, False, True,  (0,   0,   0)),
    Label('dynamic',              5,   255, 'void',         0, False, True,  (111, 74,  0)),
    Label('ground',               6,   255, 'void',         0, False, True,  (81,  0,  81)),
    Label('road',                 7,   0,   'flat',         1, False, False, (128, 64, 128)),
    Label('sidewalk',             8,   1,   'flat',         1, False, False, (244, 35, 232)),
    Label('parking',              9,   255, 'flat',         1, False, True,  (250, 170, 160)),
    Label('rail track',           10,  255, 'flat',         1, False, True,  (230, 150, 140)),
    Label('building',             11,  2,   'construction', 2, False, False, (70,  70,  70)),
    Label('wall',                 12,  3,   'construction', 2, False, False, (102, 102, 156)),
    Label('fence',                13,  4,   'construction', 2, False, False, (190, 153, 153)),
    Label('guard rail',           14,  255, 'construction', 2, False, True,  (180, 165, 180)),
    Label('bridge',               15,  255, 'construction', 2, False, True,  (150, 100, 100)),
    Label('tunnel',               16,  255, 'construction', 2, False, True,  (150, 120, 90)),
    Label('pole',                 17,  5,   'object',       3, False, False, (153, 153, 153)),
    Label('polegroup',            18,  255, 'object',       3, False, True,  (153, 153, 153)),
    Label('traffic light',        19,  6,   'object',       3, False, False, (250, 170, 30)),
    Label('traffic sign',         20,  7,   'object',       3, False, False, (220, 220, 0)),
    Label('vegetation',           21,  8,   'nature',       4, False, False, (107, 142, 35)),
    Label('terrain',              22,  9,   'nature',       4, False, False, (152, 251, 152)),
    Label('sky',                  23,  10,  'sky',          5, False, False, (70,  130, 180)),
    Label('person',               24,  11,  'human',        6, True,  False, (220, 20,  60)),
    Label('rider',                25,  12,  'human',        6, True,  False, (255, 0,   0)),
    Label('car',                  26,  13,  'vehicle',      7, True,  False, (0,   0,   142)),
    Label('truck',                27,  14,  'vehicle',      7, True,  False, (0,   0,   70)),
    Label('bus',                  28,  15,  'vehicle',      7, True,  False, (0,   60,  100)),
    Label('caravan',              29,  255, 'vehicle',      7, True,  True,  (0,   0,   90)),
    Label('trailer',              30,  255, 'vehicle',      7, True,  True,  (0,   0,   110)),
    Label('train',                31,  16,  'vehicle',      7, True,  False, (0,   80,  100)),
    Label('motorcycle',           32,  17,  'vehicle',      7, True,  False, (0,   0,   230)),
    Label('bicycle',              33,  18,  'vehicle',      7, True,  False, (119, 11,  32)),
    Label('license plate',        -1,  -1,  'vehicle',      7, False, True,  (0,   0,   142)),
]

trainId_to_name = {
        1: 'road', 2: 'sidewalk', 3: 'building', 4: 'wall', 5: 'fence',
        6: 'pole', 7: 'traffic light', 8: 'traffic sign', 9: 'vegetation',
        10: 'terrain', 11: 'sky', 12: 'person', 13: 'rider', 14: 'car',
        15: 'truck', 16: 'bus', 17: 'train', 18: 'motorcycle', 19: 'bicycle'
    }

# =============================================================================
# FILENAME PARSING UTILITIES AND DATA LOADING
# =============================================================================
def load_partition_data(partition_num, base_path,
                        img_prefix='train_images_partition_',
                        ann_prefix='instance_ann_partition_'):
    """
    Loads a single partition's images and annotations from safetensors files.
    """
    img_file = f"{img_prefix}{partition_num}.safetensors"
    ann_file = f"{ann_prefix}{partition_num}.safetensors"

    img_path = os.path.join(base_path, img_file)
    ann_path = os.path.join(base_path, ann_file)

    images_tensor = load_file(img_path)['images']
    annotations_tensor = load_file(ann_path)['instance_annotations']

    return images_tensor, annotations_tensor


def collate_fn(batch):
    return tuple(zip(*batch))


def load_data_loaders_and_datasets(split="train",
                      root_path="/content/drive/MyDrive/Vision Project/partitioned_data",
                      partitions=8, batch_size=4, num_workers=0):
    """
    Loads all partitions for a given split ('train' or 'val') into DataLoaders.
    """
    # Base path for split
    base_path = os.path.join(root_path, split)

    # Prefixes depend on split
    img_prefix = f"{split}_images_partition_"
    ann_prefix = f"instance_ann_partition_"

    datasets = []
    loaders = []

    for partition_num in range(1, partitions + 1):
        images_tensor, annotations_tensor = load_partition_data(
            partition_num, base_path, img_prefix, ann_prefix
        )

        dataset = MaskRCNNDatasetFromTensors(images_tensor, annotations_tensor)
        datasets.append(dataset)

        print(f"[{split.capitalize()}] Partition {partition_num} - "
              f"Images: {images_tensor.shape}, Annotations: {annotations_tensor.shape}")

    for i, dataset in enumerate(datasets, start=1):
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=(split=="train"),
                            num_workers=num_workers, collate_fn=collate_fn)
        loaders.append(loader)
        print(f"[{split.capitalize()} Loader] Partition {i} - {len(dataset)} samples")

    return loaders,datasets
    
def load_data_loader_and_dataset_for_partition(
    partition_num,
    split="train",
    root_path="/content/drive/MyDrive/Vision Project/partitioned_data",
    batch_size=4,
    num_workers=0
):
    """
    Loads a single partition's dataset and DataLoader for a given split ('train' or 'val').
    """
    if split == 'extra_training_from_val_set':
     base_path = os.path.join(root_path, split) # Making a ducktape special case for extra trianin lol
     img_prefix = f"train_images_partition_"
    else:
      base_path = os.path.join(root_path, split)
      img_prefix = f"{split}_images_partition_"
    ann_prefix = f"instance_ann_partition_"

    images_tensor, annotations_tensor = load_partition_data(
        partition_num, base_path, img_prefix, ann_prefix
    )

    dataset = MaskRCNNDatasetFromTensors(images_tensor, annotations_tensor)

    print(f"[{split.capitalize()}] Partition {partition_num} - "
          f"Images: {images_tensor.shape}, Annotations: {annotations_tensor.shape}")

    loader = DataLoader(
        dataset, batch_size=batch_size,
        shuffle=(split == "train"),
        num_workers=num_workers,
        collate_fn=collate_fn
    )

    print(f"[{split.capitalize()} Loader] Partition {partition_num} - {len(dataset)} samples")

    return loader, dataset


def load_partition_tensors(partition_num, base_path):
    img_path = f"{base_path}/train_images_partition_{partition_num}.safetensors"
    ann_path = f"{base_path}/instance_ann_partition_{partition_num}.safetensors"
    images_tensor = load_file(img_path)['images']
    annotations_tensor = load_file(ann_path)['instance_annotations']
    return images_tensor, annotations_tensor




# =============================================================================
# DATA INTEGRITY CHECKS AND ANALYZERS
# =============================================================================

def visualize_random_sample_with_masks(images_tensor, annotations_tensor):
   
    idx = random.randint(0, images_tensor.shape[0] - 1)

    img = images_tensor[idx].cpu().permute(1, 2, 0).numpy()
    img = (img - img.min()) / (img.max() - img.min())

    ann_map = annotations_tensor[idx].cpu().numpy().astype(np.int32)
    instance_ids = np.unique(ann_map)
    instance_ids = instance_ids[instance_ids != 0]

    plt.figure(figsize=(12, 8))
    plt.imshow(img)

    for inst_id in instance_ids:
        mask = ann_map == inst_id
        color_mask = np.zeros((*mask.shape, 4))
        color_mask[mask, :3] = np.random.rand(3)
        color_mask[mask, 3] = 0.4
        plt.imshow(color_mask)

    plt.title(f"Random Sample {idx} - {len(instance_ids)} instances")
    plt.axis('off')
    plt.show()


    
def analyze_dataset(dataset, small_thresh=32*32, medium_thresh=96*96):
   
    # Map from class id to class name (adjust based on your dataset)
    trainId_to_name = {
        1: 'road', 2: 'sidewalk', 3: 'building', 4: 'wall', 5: 'fence',
        6: 'pole', 7: 'traffic light', 8: 'traffic sign', 9: 'vegetation',
        10: 'terrain', 11: 'sky', 12: 'person', 13: 'rider', 14: 'car',
        15: 'truck', 16: 'bus', 17: 'train', 18: 'motorcycle', 19: 'bicycle'
    }

    class_counts = Counter()
    size_counts = {'small': 0, 'medium': 0, 'large': 0}
    class_size_dist = defaultdict(lambda: {'small': 0, 'medium': 0, 'large': 0})

    areas = []
    widths = []
    heights = []
    aspect_ratios = []

    print("Analyzing dataset...")

    for i, (_, target) in enumerate(dataset):
        try:
            for box, label, area in zip(target['boxes'], target['labels'], target['area']):
                label = label.item()
                area_val = area.item()
                width = (box[2] - box[0]).item()
                height = (box[3] - box[1]).item()
                aspect_ratio = width / height if height > 0 else 0

                # Update statistics
                class_counts[label] += 1
                areas.append(area_val)
                widths.append(width)
                heights.append(height)
                aspect_ratios.append(aspect_ratio)

                # Size categorization
                if area_val <= small_thresh:
                    size_counts['small'] += 1
                    class_size_dist[label]['small'] += 1
                elif area_val <= medium_thresh:
                    size_counts['medium'] += 1
                    class_size_dist[label]['medium'] += 1
                else:
                    size_counts['large'] += 1
                    class_size_dist[label]['large'] += 1

        except Exception as e:
            print(f"Skipping image {i}: {e}")
            continue

    # Convert to numpy arrays for statistics
    areas = np.array(areas)
    widths = np.array(widths)
    heights = np.array(heights)
    aspect_ratios = np.array(aspect_ratios)

    total_objects = sum(class_counts.values())

    print(f"\n=== DATASET ANALYSIS ===")
    print(f"Total objects: {total_objects}")
    print(f"Total images processed: {len(dataset)}")
    print(f"Average objects per image: {total_objects / len(dataset):.2f}")

    print(f"\n=== SIZE DISTRIBUTION ===")
    print(f"Small objects (â‰¤{int(np.sqrt(small_thresh))}x{int(np.sqrt(small_thresh))}px): {size_counts['small']} ({size_counts['small']/total_objects*100:.1f}%)")
    print(f"Medium objects ({int(np.sqrt(small_thresh))}-{int(np.sqrt(medium_thresh))}px): {size_counts['medium']} ({size_counts['medium']/total_objects*100:.1f}%)")
    print(f"Large objects (>{int(np.sqrt(medium_thresh))}x{int(np.sqrt(medium_thresh))}px): {size_counts['large']} ({size_counts['large']/total_objects*100:.1f}%)")

    print(f"\n=== OBJECT DIMENSIONS ===")
    print(f"Area stats - Mean: {areas.mean():.1f}, Median: {np.median(areas):.1f}, Std: {areas.std():.1f}")
    print(f"Width stats - Mean: {widths.mean():.1f}, Median: {np.median(widths):.1f}, Min: {widths.min():.1f}, Max: {widths.max():.1f}")
    print(f"Height stats - Mean: {heights.mean():.1f}, Median: {np.median(heights):.1f}, Min: {heights.min():.1f}, Max: {heights.max():.1f}")
    print(f"Aspect ratio stats - Mean: {aspect_ratios.mean():.2f}, Median: {np.median(aspect_ratios):.2f}, Min: {aspect_ratios.min():.2f}, Max: {aspect_ratios.max():.2f}")

    print(f"\n=== CLASS DISTRIBUTION ===")
    for class_id, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True):
        class_name = trainId_to_name.get(class_id, f"class_{class_id}")
        print(f"{class_name:<15}: {count:>6} ({count/total_objects*100:>5.1f}%)")

    print(f"\n=== SMALL OBJECT ANALYSIS ===")
    if size_counts['small'] > 0:
        mask_small = areas <= small_thresh
        small_widths = widths[mask_small]
        small_heights = heights[mask_small]
        small_areas = areas[mask_small]

        print(f"Small object dimensions:")
        print(f"  Width: {small_widths.mean():.1f}Â±{small_widths.std():.1f} (min: {small_widths.min():.1f}, max: {small_widths.max():.1f})")
        print(f"  Height: {small_heights.mean():.1f}Â±{small_heights.std():.1f} (min: {small_heights.min():.1f}, max: {small_heights.max():.1f})")
        print(f"  Area: {small_areas.mean():.1f}Â±{small_areas.std():.1f}")

        print(f"\nSmall objects per class:")
        for class_id, count in sorted(class_counts.items(), key=lambda x: class_size_dist[x[0]]['small'], reverse=True):
            small_count = class_size_dist[class_id]['small']
            if small_count > 0:
                class_name = trainId_to_name.get(class_id, f"class_{class_id}")
                print(f"  {class_name:<15}: {small_count:>4}/{count:>4} ({small_count/count*100:>5.1f}%)")
    else:
        print("No small objects found!")

    return {
        'class_counts': class_counts,
        'size_counts': size_counts,
        'areas': areas,
        'widths': widths,
        'heights': heights,
        'aspect_ratios': aspect_ratios,
        'total_objects': total_objects,
        'class_size_dist': class_size_dist
    }

# =============================================================================
# LABEL MAPPING UTILITIES
# =============================================================================

# Convert label IDs to training IDs
def id_to_trainId(id_num):
    return id_to_trainId_map.get(id_num, None)

# =============================================================================
# DATASET CLASSES
# =============================================================================
        
class MaskRCNNDatasetFromTensors(Dataset):
    def __init__(self, images_tensor, annotations_tensor, id_to_trainId=None):
        assert images_tensor.shape[0] == annotations_tensor.shape[0], "Images and annotations must have the same number of samples"
        self.images = images_tensor
        self.annotations = annotations_tensor.squeeze(1)  # [N, H, W]

        if id_to_trainId is None:
            self.id_to_trainId = {
                7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,
                19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11,
                25: 12, 26: 13, 27: 14, 28: 15, 31: 16,
                32: 17, 33: 18
            }
        else:
            self.id_to_trainId = id_to_trainId

    def __len__(self):
        return self.images.shape[0]

    def __getitem__(self, idx):
        image_tensor = self.images[idx]  # [3, H, W]

        # Reverse normalization: multiply by 255 and round to get original labels
        annotation_float = self.annotations[idx]  # [H, W], float tensor normalized 0-1
        annotation_recovered = (annotation_float * 255).round().to(torch.int64).cpu().numpy()

        masks = []
        boxes = []
        labels = []

        for instance_id in np.unique(annotation_recovered):
            if instance_id < 1000:
                continue

            label_id = instance_id // 1000
            if label_id not in self.id_to_trainId:
                continue

            train_id = self.id_to_trainId[label_id]
            binary_mask = (annotation_recovered == instance_id).astype(np.uint8)

            if binary_mask.sum() == 0:
                continue

            pos = np.where(binary_mask)
            xmin, xmax = np.min(pos[1]), np.max(pos[1])
            ymin, ymax = np.min(pos[0]), np.max(pos[0])

            if xmax <= xmin or ymax <= ymin:
                continue

            boxes.append([xmin, ymin, xmax, ymax])
            masks.append(binary_mask)
            labels.append(train_id + 1)  # +1 for background=0

        if len(boxes) == 0:
            boxes_tensor = torch.zeros((0, 4), dtype=torch.float32)
            labels_tensor = torch.zeros((0,), dtype=torch.int64)
            masks_tensor = torch.zeros((0, annotation_recovered.shape[0], annotation_recovered.shape[1]), dtype=torch.uint8)
        else:
            boxes_tensor = torch.tensor(boxes, dtype=torch.float32)
            labels_tensor = torch.tensor(labels, dtype=torch.int64)
            masks_tensor = torch.tensor(np.stack(masks), dtype=torch.uint8)

        target = {
            "boxes": boxes_tensor,
            "labels": labels_tensor,
            "masks": masks_tensor,
            "image_id": torch.tensor([idx]),
            "area": (boxes_tensor[:, 3] - boxes_tensor[:, 1]) * (boxes_tensor[:, 2] - boxes_tensor[:, 0]) if boxes_tensor.shape[0] > 0 else torch.tensor([]),
            "iscrowd": torch.zeros((labels_tensor.shape[0],), dtype=torch.int64)
        }

        return image_tensor, target
        
        
# =============================================================================
# TRAINING FUNCTIONS
# =============================================================================

def train_epoch_alpha_mixed_precision(
    model, optimizer, device, alpha=0.5, iou_threshold=0.5, max_epochs=100, patience=10,
    root_path="/content/drive/MyDrive/Vision Project/partitioned_data", train_partitions=2,extra_training=False,
    val_partitions=[True, False, True, False], batch_size=4, num_workers=0,
    cosine_t_max=None, cosine_eta_min=1e-6, cosine_last_epoch=-1, use_wandb=False,
    pruning=True, trial=None, checkpoint_save_path="./checkpoints", load_checkpoint=False,
    validate_only_last=False
):
    import os
    import math

    os.makedirs(checkpoint_save_path, exist_ok=True)
    scaler = GradScaler()
    best_val_loss = float('inf')
    patience_counter = 0
    start_epoch = 0
    train_losses = []
    val_losses = []
    val_maps = []

    # --- Cosine Annealing scheduler ---
    from torch.optim.lr_scheduler import CosineAnnealingLR
    if cosine_t_max is None:
        cosine_t_max = max_epochs
    scheduler = CosineAnnealingLR(optimizer, T_max=cosine_t_max, eta_min=cosine_eta_min, last_epoch=cosine_last_epoch)

    # --- Load checkpoint if path is provided ---
    if load_checkpoint:
        print(f"[Info] Loading checkpoint from {load_checkpoint} ...")
        checkpoint = torch.load(load_checkpoint, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        if 'scaler_state_dict' in checkpoint:
            scaler.load_state_dict(checkpoint['scaler_state_dict'])
        best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        start_epoch = checkpoint.get('epoch', 0) + 1
        train_losses = checkpoint.get('train_losses', [])
        val_losses = checkpoint.get('val_losses', [])
        val_maps = checkpoint.get('val_maps', [])
        print(f"[Info] Resuming from epoch {start_epoch}")

    for epoch in range(start_epoch, max_epochs):
        print(f"\n[Epoch {epoch+1}/{max_epochs}] Starting training...")
        model.train()
        total_loss = 0.0
        total_batches = 0
        if extra_training:
            train_partitions = 2
            print("Extra training activated.")
        # --- Training ---
        for partition_num in range(1, train_partitions + 1):
            if extra_training:
             train_loader, train_dataset = load_data_loader_and_dataset_for_partition(
                partition_num, split="extra_training_from_val_set", root_path=root_path, batch_size=batch_size, num_workers=num_workers
            )
            else:
                train_loader, train_dataset = load_data_loader_and_dataset_for_partition(
                    partition_num, split="train", root_path=root_path, batch_size=batch_size, num_workers=num_workers
                )

            for images, targets in tqdm(train_loader, desc=f"Training epoch {epoch+1} partition {partition_num}"):
                images = [img.to(device) for img in images]
                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

                optimizer.zero_grad()

                with autocast():
                    losses_dict = model(images, targets)
                    normal_loss = sum(loss for loss in losses_dict.values())

                # GIoU loss
                model.eval()
                with torch.no_grad():
                    outputs = model(images)
                model.train()

                giou_losses = []
                for output, target in zip(outputs, targets):
                    pred_boxes = output['boxes']
                    gt_boxes = target['boxes']

                    if pred_boxes.size(0) == 0 or gt_boxes.size(0) == 0:
                        giou_losses.append(torch.tensor(0.0, device=device, requires_grad=True))
                        continue

                    pred_indices, target_indices = match_boxes_hungarian(pred_boxes, gt_boxes, iou_threshold)

                    if len(pred_indices) == 0:
                        giou_losses.append(torch.tensor(0.0, device=device, requires_grad=True))
                        continue

                    matched_pred = pred_boxes[pred_indices]
                    matched_gt = gt_boxes[target_indices]
                    giou_matrix = box_ops.generalized_box_iou(matched_pred, matched_gt)
                    giou_losses.append((1 - torch.diag(giou_matrix)).mean())

                giou_loss_value = torch.stack(giou_losses).mean() if giou_losses else torch.tensor(0.0, device=device, requires_grad=True)
                total_loss_value = alpha * normal_loss + (1 - alpha) * giou_loss_value

                scaler.scale(total_loss_value).backward()
                scaler.step(optimizer)
                scaler.update()

                total_loss += total_loss_value.item()
                total_batches += 1

                del images, targets, outputs, losses_dict, giou_losses, total_loss_value
                torch.cuda.empty_cache()

            del train_loader, train_dataset
            gc.collect()
            torch.cuda.empty_cache()

        avg_train_loss = total_loss / total_batches if total_batches > 0 else 0.0
        train_losses.append(avg_train_loss)
        print(f"[Epoch {epoch+1}] Avg Train Loss: {avg_train_loss:.4f}")

        # --- Validation ---
        # Only validate if validate_only_last is False OR if this is the last epoch
        if not validate_only_last or epoch == max_epochs - 1:
            model.eval()
            total_val_loss = 0.0
            total_val_batches = 0

            selected_partitions = [i+1 for i, selected in enumerate(val_partitions) if selected]

            for partition_num in selected_partitions:
                val_loader, val_dataset = load_data_loader_and_dataset_for_partition(
                    partition_num, split="val", root_path=root_path, batch_size=batch_size, num_workers=num_workers
                )
                last_partition_num = partition_num

                with torch.no_grad():
                    for images, targets in tqdm(val_loader, desc=f"Validating epoch {epoch+1} partition {partition_num}"):
                        images = [img.to(device) for img in images]
                        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

                        with autocast():
                            model.train()
                            losses_dict = model(images, targets)
                            normal_loss = sum(loss for loss in losses_dict.values())

                            model.eval()
                            outputs = model(images)

                        giou_losses = []
                        for output, target in zip(outputs, targets):
                            pred_boxes = output['boxes']
                            gt_boxes = target['boxes']

                            if pred_boxes.size(0) == 0 or gt_boxes.size(0) == 0:
                                giou_losses.append(torch.tensor(0.0, device=device))
                                continue

                            pred_indices, target_indices = match_boxes_hungarian(pred_boxes, gt_boxes, iou_threshold)

                            if len(pred_indices) == 0:
                                giou_losses.append(torch.tensor(0.0, device=device))
                                continue

                            matched_pred = pred_boxes[pred_indices]
                            matched_gt = gt_boxes[target_indices]
                            giou_matrix = box_ops.generalized_box_iou(matched_pred, matched_gt)
                            giou_losses.append((1 - torch.diag(giou_matrix)).mean())

                        giou_loss_value = torch.stack(giou_losses).mean() if giou_losses else torch.tensor(0.0, device=device)
                        total_loss_value = alpha * normal_loss + (1 - alpha) * giou_loss_value

                        total_val_loss += total_loss_value.item()
                        total_val_batches += 1

                        del images, targets, outputs, losses_dict, giou_losses, total_loss_value
                        torch.cuda.empty_cache()

                if partition_num == selected_partitions[-1]:
                    overall_map = evaluate_map_extended_hungarian(model, val_loader, device, iou_threshold=iou_threshold)
                    print(f"[Epoch {epoch+1}] Overall mAP (last val partition {partition_num}): {overall_map['map']:.4f}")

                    if trial is not None:
                        trial.report(overall_map['map'], epoch)
                        if trial.should_prune():
                            raise optuna.TrialPruned()

                    val_maps.append(overall_map['map'])

                del val_loader, val_dataset
                gc.collect()
                torch.cuda.empty_cache()

            avg_val_loss = total_val_loss / total_val_batches if total_val_batches > 0 else float('inf')
            val_losses.append(avg_val_loss)
            print(f"[Epoch {epoch+1}] Avg Val Loss: {avg_val_loss:.4f}")

            if use_wandb:
                wandb.log({
                    "epoch": epoch + 1,
                    "train_loss": avg_train_loss,
                    "val_loss": avg_val_loss,
                    "overall_map": overall_map,
                    "learning_rate": optimizer.param_groups[0]['lr']
                })

            # --- Early stopping logic ---
            min_delta = 0.002
            gap_threshold = 0.5
            improved = False

            if avg_val_loss < best_val_loss - min_delta:
                best_val_loss = avg_val_loss
                patience_counter = 0
                improved = True
                print(f"[Info] Validation loss improved. Resetting patience counter.")
            elif avg_val_loss - avg_train_loss > gap_threshold:
                patience_counter += 1
                print(f"[Info] Gap between train and val loss is too large. Patience: {patience_counter}/{patience}")
            else:
                patience_counter += 1
                print(f"[Info] Validation loss did not improve enough. Patience: {patience_counter}/{patience}")

            if patience_counter >= patience:
                print(f"[Early Stopping] Stopping at epoch {epoch+1}")
                break

        else:
            # If skipping validation, append placeholder values
            val_losses.append(None)
            val_maps.append(None)

            if use_wandb:
                wandb.log({
                    "epoch": epoch + 1,
                    "train_loss": avg_train_loss,
                    "learning_rate": optimizer.param_groups[0]['lr']
                })

        # --- Cosine scheduler step ---
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        print(f"[Epoch {epoch+1}] Cosine LR: {current_lr:.6f}")

        # --- Checkpoint save every 2 epochs ---
        if (epoch + 1) % 2 == 0:
            checkpoint_file = os.path.join(checkpoint_save_path, f"checkpoint_epoch_{epoch+1}.pth")
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'scaler_state_dict': scaler.state_dict(),
                'best_val_loss': best_val_loss,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'val_maps': val_maps
            }, checkpoint_file)
            print(f"[Checkpoint] Saved checkpoint to {checkpoint_file}")

    return val_losses, train_losses, val_maps


# =============================================================================
# DATASET VALIDATION AND ANALYSIS UTILITIES
# =============================================================================

def analyze_dataset(dataset, small_thresh=32*32, medium_thresh=96*96):


    # Reverse mapping for class names (adjust based on your classes)
    trainId_to_name = {
    1: 'road',
    2: 'sidewalk',
    3: 'building',
    4: 'wall',
    5: 'fence',
    6: 'pole',
    7: 'traffic light',
    8: 'traffic sign',
    9: 'vegetation',
    10: 'terrain',
    11: 'sky',
    12: 'person',
    13: 'rider',
    14: 'car',
    15: 'truck',
    16: 'bus',
    17: 'train',
    18: 'motorcycle',
    19: 'bicycle'
}

    class_counts = Counter()
    size_counts = {'small': 0, 'medium': 0, 'large': 0}
    class_size_dist = defaultdict(lambda: {'small': 0, 'medium': 0, 'large': 0})

    areas = []
    widths = []
    heights = []

    print("Analyzing dataset...")

    for i in range(len(dataset)):
        try:
            target = dataset[i][1]  # Get target dict

            for j, (box, label, area) in enumerate(zip(target['boxes'], target['labels'], target['area'])):
                # Class distribution
                class_counts[label.item()] += 1

                # Size analysis
                area_val = area.item()
                width = (box[2] - box[0]).item()
                height = (box[3] - box[1]).item()

                areas.append(area_val)
                widths.append(width)
                heights.append(height)

                # Categorize by size
                if area_val <= small_thresh:
                    size_counts['small'] += 1
                    class_size_dist[label.item()]['small'] += 1
                elif area_val <= medium_thresh:
                    size_counts['medium'] += 1
                    class_size_dist[label.item()]['medium'] += 1
                else:
                    size_counts['large'] += 1
                    class_size_dist[label.item()]['large'] += 1

        except Exception as e:
            print(f"Skipping image {i}: {e}")
            continue

    # Convert to numpy for stats
    areas = np.array(areas)
    widths = np.array(widths)
    heights = np.array(heights)

    # Print results
    total_objects = sum(class_counts.values())

    print(f"\n=== DATASET ANALYSIS ===")
    print(f"Total objects: {total_objects}")
    print(f"Total images processed: {len(dataset)}")
    print(f"Average objects per image: {total_objects/len(dataset):.2f}")

    print(f"\n=== SIZE DISTRIBUTION ===")
    print(f"Small objects (Ã¢â€°Â¤{int(np.sqrt(small_thresh))}x{int(np.sqrt(small_thresh))}px): {size_counts['small']} ({size_counts['small']/total_objects*100:.1f}%)")
    print(f"Medium objects ({int(np.sqrt(small_thresh))}-{int(np.sqrt(medium_thresh))}px): {size_counts['medium']} ({size_counts['medium']/total_objects*100:.1f}%)")
    print(f"Large objects (>{int(np.sqrt(medium_thresh))}x{int(np.sqrt(medium_thresh))}px): {size_counts['large']} ({size_counts['large']/total_objects*100:.1f}%)")

    print(f"\n=== OBJECT DIMENSIONS ===")
    print(f"Area stats - Mean: {areas.mean():.1f}, Median: {np.median(areas):.1f}, Std: {areas.std():.1f}")
    print(f"Width stats - Mean: {widths.mean():.1f}, Median: {np.median(widths):.1f}, Min: {widths.min():.1f}, Max: {widths.max():.1f}")
    print(f"Height stats - Mean: {heights.mean():.1f}, Median: {np.median(heights):.1f}, Min: {heights.min():.1f}, Max: {heights.max():.1f}")

    print(f"\n=== CLASS DISTRIBUTION ===")
    sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)
    for class_id, count in sorted_classes:
        class_name = trainId_to_name.get(class_id, f"class_{class_id}")
        percentage = count / total_objects * 100
        print(f"{class_name:<15}: {count:>6} ({percentage:>5.1f}%)")

    print(f"\n=== SMALL OBJECT ANALYSIS ===")
    if size_counts['small'] > 0:
        small_areas = areas[areas <= small_thresh]
        small_widths = widths[np.array(areas) <= small_thresh]
        small_heights = heights[np.array(areas) <= small_thresh]

        print(f"Small object dimensions:")
        print(f"  Width: {small_widths.mean():.1f}Ã‚Â±{small_widths.std():.1f} (min: {small_widths.min():.1f}, max: {small_widths.max():.1f})")
        print(f"  Height: {small_heights.mean():.1f}Ã‚Â±{small_heights.std():.1f} (min: {small_heights.min():.1f}, max: {small_heights.max():.1f})")
        print(f"  Area: {small_areas.mean():.1f}Ã‚Â±{small_areas.std():.1f}")

        print(f"\nSmall objects per class:")
        for class_id, count in sorted(class_counts.items(), key=lambda x: class_size_dist[x[0]]['small'], reverse=True):
            small_count = class_size_dist[class_id]['small']
            if small_count > 0:
                class_name = trainId_to_name.get(class_id, f"class_{class_id}")
                small_pct = small_count / count * 100
                print(f"  {class_name:<15}: {small_count:>4}/{count:>4} ({small_pct:>5.1f}%)")
    else:
        print("No small objects found!")

    return {
        'class_counts': class_counts,
        'size_counts': size_counts,
        'areas': areas,
        'widths': widths,
        'heights': heights,
        'total_objects': total_objects
    }

# =============================================================================
# DATALOADER UTILITIES
# =============================================================================

# Custom collate function for Mask R-CNN
def collate_fn(batch):
    images, targets = tuple(zip(*batch))
    return list(images), list(targets)

# =============================================================================
# MODEL LOADING, INFERENCE AND VISUALIZATION
# =============================================================================
def load_model_correctly(checkpoint_path, num_classes=20, load_heads=True, freeze_backbone=False):
    """Load trained Mask R-CNN model correctly"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    checkpoint = torch.load(checkpoint_path, map_location=device)

    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
    elif 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    elif 'model' in checkpoint:
        state_dict = checkpoint['model']
    else:
        state_dict = checkpoint

    model = maskrcnn_resnet50_fpn(pretrained=True)

    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    in_channels = model.roi_heads.mask_predictor.conv5_mask.in_channels
    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_channels, 256, num_classes)

    model_keys = set(model.state_dict().keys())
    checkpoint_keys = set(state_dict.keys())
    matching_keys = model_keys.intersection(checkpoint_keys)

    if load_heads and len(matching_keys) > len(model_keys) * 0.8:
        try:
            model.load_state_dict(state_dict, strict=False)
        except Exception:
            load_heads = False
    else:
        load_heads = False

    if not load_heads:
        backbone_body_weights = {}
        backbone_fpn_weights = {}
        rpn_weights = {}

        for key, value in state_dict.items():
            if key.startswith('backbone.body.'):
                new_key = key.replace('backbone.body.', '')
                backbone_body_weights[new_key] = value
            elif key.startswith('backbone.fpn.'):
                new_key = key.replace('backbone.fpn.', '')
                backbone_fpn_weights[new_key] = value
            elif key.startswith('rpn.'):
                rpn_weights[key] = value

        if backbone_body_weights:
            try:
                model.backbone.body.load_state_dict(backbone_body_weights, strict=False)
            except Exception:
                pass

        if backbone_fpn_weights:
            try:
                model.backbone.fpn.load_state_dict(backbone_fpn_weights, strict=False)
            except Exception:
                pass

        if rpn_weights:
            try:
                model.rpn.load_state_dict(rpn_weights, strict=False)
            except Exception:
                pass

    if freeze_backbone:
        for param in model.backbone.parameters():
            param.requires_grad = False
        model.backbone.eval()

    model.to(device)
    model.eval()
    return model
    
    
def analyze_backbone_percent_difference(checkpoint_path, device="cpu", epsilon=1e-8):
    """
    Compare transferred checkpoint backbone vs torchvision DeepLabV3 ResNet-50 backbone.
    Shows differences as percentage relative to DeepLab weights.
    """
    # Load checkpoint
    ckpt = torch.load(checkpoint_path, map_location=device)
    state_dict = ckpt.get('model_state_dict', ckpt)

    # Extract only backbone weights from checkpoint
    ckpt_backbone = {k: v for k, v in state_dict.items() if "backbone" in k}
    norm_ckpt = {k.replace("backbone.body.", "").replace("backbone.", ""): v for k, v in ckpt_backbone.items()}

    # Load DeepLab backbone
    deeplab = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True).backbone
    deeplab_state = deeplab.state_dict()

    ckpt_keys = set(norm_ckpt.keys())
    deeplab_keys = set(deeplab_state.keys())
    common = ckpt_keys.intersection(deeplab_keys)

    # Compute percent differences
    group_diffs = defaultdict(list)
    sample_diffs = {}
    for k in sorted(common):
        t1 = norm_ckpt[k].detach().to(device)
        t2 = deeplab_state[k].detach().to(device)
        diff_percent = torch.mean(torch.abs(t1 - t2) / (torch.abs(t2) + epsilon)) * 100.0

        group = k.split('.')[0]
        group_diffs[group].append(diff_percent.item())
        sample_diffs[k] = diff_percent.item()

    # Display grouped results
    print(f"âœ… Common keys: {len(common)}")
    for group, diffs in group_diffs.items():
        avg_diff = sum(diffs) / len(diffs)
        print(f"ðŸ“‚ {group:<8} | keys: {len(diffs):<4} | avg % diff: {avg_diff:.2f}%")

    # Show a few sample diffs
    print("\nðŸ”Ž Sample percent differences (first 10):")
    for k in list(sample_diffs.keys())[:10]:
        print(f"  {k:<40} diff={sample_diffs[k]:.2f}%")

    # Plot bar chart
    groups, avgs = zip(*[(g, sum(d)/len(d)) for g, d in group_diffs.items()])
    plt.figure(figsize=(8,4))
    plt.bar(groups, avgs)
    plt.title("Average Percent Difference per Backbone Stage")
    plt.ylabel("Mean Percent Difference (%)")
    plt.xlabel("Backbone Stage")
    plt.show()
    
    
def count_parameters(model):
    """Count parameters in each module and show frozen status"""
    print(f"{'Module':<30} {'Params':>10} {'Trainable':>10} {'Status'}")
    print("-" * 60)

    total_params = 0
    trainable_params = 0

    for name, module in model.named_children():
        module_params = sum(p.numel() for p in module.parameters())
        module_trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)

        status = "FROZEN" if module_trainable == 0 else "TRAINABLE"

        print(f"{name:<30} {module_params:>10,} {module_trainable:>10,} {status}")

        total_params += module_params
        trainable_params += module_trainable

    print("-" * 60)
    print(f"{'TOTAL':<30} {total_params:>10,} {trainable_params:>10,}")
    print(f"Frozen parameters: {total_params - trainable_params:,}")
    print(f"Training {trainable_params/total_params*100:.1f}% of parameters")


# Alternative function to just inspect your checkpoint
def inspect_checkpoint(checkpoint_path):
    """Inspect checkpoint structure without loading model"""
    print(f"Ã°Å¸â€Â Inspecting checkpoint: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    print(f"Ã°Å¸â€œâ€š Checkpoint type: {type(checkpoint)}")
    print(f"Ã°Å¸â€œâ€š Top-level keys: {list(checkpoint.keys()) if isinstance(checkpoint, dict) else 'Not a dict'}")

    # Try to find the actual state dict
    state_dict = None
    if isinstance(checkpoint, dict):
        for key in ['model_state_dict', 'state_dict', 'model']:
            if key in checkpoint:
                state_dict = checkpoint[key]
                print(f"Ã¢Å“â€¦ Found state dict under key: '{key}'")
                break
        if state_dict is None:
            state_dict = checkpoint
            print("Ã¢Å“â€¦ Using checkpoint as state dict")
    else:
        state_dict = checkpoint
        print("Ã¢Å“â€¦ Checkpoint is directly a state dict")

    if isinstance(state_dict, dict):
        print(f"Ã°Å¸â€œÅ  State dict has {len(state_dict)} parameters")
        print(f"Ã°Å¸â€Â Parameter keys (first 10):")
        for i, key in enumerate(list(state_dict.keys())[:10]):
            tensor_shape = state_dict[key].shape if hasattr(state_dict[key], 'shape') else 'No shape'
            print(f"  {i+1}. {key}: {tensor_shape}")

        # Check for common components
        components = ['backbone', 'rpn', 'roi_heads', 'classifier', 'box_predictor', 'mask_predictor']
        found_components = []
        for comp in components:
            if any(comp in key for key in state_dict.keys()):
                found_components.append(comp)
        print(f"Ã°Å¸Å½Â¯ Found components: {found_components}")

    return checkpoint

    
def run_inference(model, dataset, idx, device='cuda'):
    model.eval()
    model.to(device)

    # Get one image and move to device
    image_tensor, _ = dataset[idx]
    image_tensor = image_tensor.to(device)

    # Remove batch dimension if present - model expects [C, H, W]
    if image_tensor.dim() == 4:
        image_tensor = image_tensor.squeeze(0)

    with torch.no_grad():
        # Pass as list of 3D tensors
        prediction = model([image_tensor])

    return prediction[0], image_tensor.cpu()


def visualize_prediction(image_tensor, prediction, score_threshold=0.5):
    image = image_tensor.permute(1, 2, 0).numpy()
    masks = prediction['masks'] > 0.5
    boxes = prediction['boxes']
    labels = prediction['labels']
    scores = prediction['scores']

    plt.figure(figsize=(10, 10))
    plt.imshow(image)
    ax = plt.gca()

    for i in range(len(masks)):
        if scores[i] < score_threshold:
            continue

        mask = masks[i][0].cpu().numpy()
        color = np.random.rand(3)
        masked = np.ma.masked_where(mask == 0, mask)
        ax.imshow(masked, alpha=0.5, cmap='viridis')

        box = boxes[i].cpu().numpy()
        label_id = labels[i].item()
        label_name = trainId_to_name.get(label_id, "unknown")

        ax.add_patch(plt.Rectangle(
            (box[0], box[1]), box[2]-box[0], box[3]-box[1],
            fill=False, edgecolor='red', linewidth=2
        ))
        ax.text(
            box[0], box[1] - 5,
            f"{label_id}: {label_name}",
            color='white', fontsize=10, backgroundcolor='red'
        )

    plt.axis('off')
    plt.show()
def visualize_ground_truth(dataset, idx):
    image_tensor, target = dataset[idx]

    # Convert image to HWC format for plotting
    image = image_tensor.permute(1, 2, 0).numpy()

    boxes = target['boxes']
    labels = target['labels']
    masks = target['masks']

    plt.figure(figsize=(10, 10))
    plt.imshow(image)
    ax = plt.gca()

    for i in range(len(masks)):
        mask = masks[i].numpy()
        color = np.random.rand(3)
        masked = np.ma.masked_where(mask == 0, mask)
        ax.imshow(masked, alpha=0.5, cmap='gray')

        box = boxes[i].numpy()
        label = labels[i].item()

        ax.add_patch(plt.Rectangle(
            (box[0], box[1]), box[2]-box[0], box[3]-box[1],
            fill=False, edgecolor='lime', linewidth=2
        ))

        ax.text(box[0], box[1] - 5, f"GT Label: {label}",
                color='white', fontsize=10, backgroundcolor='green')

    plt.axis('off')
    plt.title(f"Ground Truth for Sample {idx}")
    plt.show()
    
# =============================================================================
# EVALUATION AND PERFORMANCE METRICS
# =============================================================================    
# functions for advanced analysis of validation set results



def match_boxes_hungarian(pred_boxes, target_boxes, iou_threshold=0.5):
 
    if pred_boxes.numel() == 0 or target_boxes.numel() == 0:
        return [], []

    # Compute cost matrix as 1 - GIoU because Hungarian solves min cost
    giou_matrix = generalized_box_iou(pred_boxes, target_boxes)
    cost_matrix = 1 - giou_matrix.cpu().numpy()

    pred_indices, target_indices = linear_sum_assignment(cost_matrix)

    # Filter matches with GIoU below threshold (or equivalently cost above 1 - threshold)
    valid_matches = []
    for p_idx, t_idx in zip(pred_indices, target_indices):
        if giou_matrix[p_idx, t_idx] >= iou_threshold:
            valid_matches.append((p_idx, t_idx))

    if len(valid_matches) == 0:
        return [], []

    pred_matched, target_matched = zip(*valid_matches)
    return list(pred_matched), list(target_matched)

def category_wise_metrics(preds, targets):
    metric = MeanAveragePrecision(class_metrics=True)
    metric.update(preds, targets)
    results = metric.compute()
    return results['map_per_class'], results['mar_100_per_class']

def evaluate_map_extended_hungarian(model, dataloaders, device, iou_threshold=0.5):
    model.eval()
    map_metric = MeanAveragePrecision()

    total_iou = 0.0
    total_giou = 0.0
    total_matched_pairs = 0

    all_preds = []
    all_targets = []

    # Ensure we always have a list of dataloaders
    if not isinstance(dataloaders, list):
        dataloaders = [dataloaders]

    with torch.no_grad():
        for dataloader in dataloaders:
            for images, targets in tqdm(dataloader, desc="Evaluating"):
                images = [img.to(device) for img in images]
                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

                predictions = model(images)

                preds = []
                target_list = []

                for pred, target in zip(predictions, targets):
                    pred_boxes = pred['boxes'].cpu()
                    target_boxes = target['boxes'].cpu()

                    if pred_boxes.numel() > 0 and target_boxes.numel() > 0:
                        pred_indices, target_indices = match_boxes_hungarian(pred_boxes, target_boxes, iou_threshold)

                        if len(pred_indices) > 0:
                            matched_pred_boxes = pred_boxes[pred_indices]
                            matched_target_boxes = target_boxes[target_indices]

                            giou_vals = torch.tensor([
                                generalized_box_iou(pb.unsqueeze(0), tb.unsqueeze(0)).item()
                                for pb, tb in zip(matched_pred_boxes, matched_target_boxes)
                            ])

                            iou_vals = torch.tensor([
                                box_iou(pb.unsqueeze(0), tb.unsqueeze(0)).item()
                                for pb, tb in zip(matched_pred_boxes, matched_target_boxes)
                            ])

                            total_giou += giou_vals.sum().item()
                            total_iou += iou_vals.sum().item()
                            total_matched_pairs += len(giou_vals)

                    preds.append({
                        'boxes': pred_boxes,
                        'scores': pred['scores'].cpu(),
                        'labels': pred['labels'].cpu()
                    })
                    target_list.append({
                        'boxes': target_boxes,
                        'labels': target['labels'].cpu()
                    })

                map_metric.update(preds, target_list)
                all_preds.extend(preds)
                all_targets.extend(target_list)

    map_results = map_metric.compute()

    avg_iou = total_iou / total_matched_pairs if total_matched_pairs > 0 else 0
    avg_giou = total_giou / total_matched_pairs if total_matched_pairs > 0 else 0

    map_results['avg_iou'] = avg_iou
    map_results['avg_giou'] = avg_giou

    # Compute category-wise metrics on the whole dataset
    map_per_class, mar_100_per_class = category_wise_metrics(all_preds, all_targets)
    map_results['map_per_class'] = map_per_class
    map_results['mar_100_per_class'] = mar_100_per_class

    return map_results

def evaluate_test_set(model, dataloaders, device, iou_threshold=0.5):
   
    model.eval()
    map_metric = MeanAveragePrecision()

    total_iou = 0.0
    total_giou = 0.0
    total_matched_pairs = 0

    all_preds = []
    all_targets = []

    # Ensure dataloaders is always a list
    if not isinstance(dataloaders, list):
        dataloaders = [dataloaders]

    with torch.no_grad():
        for dataloader in dataloaders:
            for images, targets in tqdm(dataloader, desc="Evaluating"):
                images = [img.to(device) for img in images]
                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

                predictions = model(images)

                preds = []
                target_list = []

                for pred, target in zip(predictions, targets):
                    pred_boxes = pred['boxes'].cpu()
                    target_boxes = target['boxes'].cpu()

                    if pred_boxes.numel() > 0 and target_boxes.numel() > 0:
                        pred_indices, target_indices = match_boxes_hungarian(
                            pred_boxes, target_boxes, iou_threshold
                        )

                        if len(pred_indices) > 0:
                            matched_pred_boxes = pred_boxes[pred_indices]
                            matched_target_boxes = target_boxes[target_indices]

                            giou_vals = torch.tensor([
                                generalized_box_iou(pb.unsqueeze(0), tb.unsqueeze(0)).item()
                                for pb, tb in zip(matched_pred_boxes, matched_target_boxes)
                            ])

                            iou_vals = torch.tensor([
                                box_iou(pb.unsqueeze(0), tb.unsqueeze(0)).item()
                                for pb, tb in zip(matched_pred_boxes, matched_target_boxes)
                            ])

                            total_giou += giou_vals.sum().item()
                            total_iou += iou_vals.sum().item()
                            total_matched_pairs += len(giou_vals)

                    preds.append({
                        'boxes': pred_boxes,
                        'scores': pred['scores'].cpu(),
                        'labels': pred['labels'].cpu()
                    })
                    target_list.append({
                        'boxes': target_boxes,
                        'labels': target['labels'].cpu()
                    })

                map_metric.update(preds, target_list)
                all_preds.extend(preds)
                all_targets.extend(target_list)

    # Global metrics
    map_results = map_metric.compute()

    avg_iou = total_iou / total_matched_pairs if total_matched_pairs > 0 else 0
    avg_giou = total_giou / total_matched_pairs if total_matched_pairs > 0 else 0

    map_results['avg_iou'] = avg_iou
    map_results['avg_giou'] = avg_giou

    # Category-wise metrics
    map_per_class, mar_100_per_class = category_wise_metrics(all_preds, all_targets)
    map_results['map_per_class'] = map_per_class
    map_results['mar_100_per_class'] = mar_100_per_class

    # --- Pretty Printing ---
    print("\n===== Evaluation Results =====")
    print(f"mAP @ IoU=0.50:0.95 : {map_results['map']:.4f}")
    print(f"mAP @ IoU=0.50    : {map_results['map_50']:.4f}")
    print(f"mAP @ IoU=0.75    : {map_results['map_75']:.4f}")
    print(f"mAR @ 100         : {map_results['mar_100']:.4f}")
    print(f"Avg IoU (Hungarian)  : {avg_iou:.4f}")
    print(f"Avg GIoU (Hungarian) : {avg_giou:.4f}")

    print("\n--- Per-Class Metrics ---")
    if isinstance(map_per_class, torch.Tensor):
        for cls_id in range(len(map_per_class)):
            class_name = trainId_to_name.get(cls_id+12, "unknown")
            print(f"Class {cls_id} ({class_name}): "
                  f"AP={map_per_class[cls_id].item():.4f}, "
                  f"AR@100={mar_100_per_class[cls_id].item():.4f}")
    else:
        for cls_id in map_per_class.keys():
            class_name = trainId_to_name.get(cls_id, "unknown")
            print(f"Class {cls_id} ({class_name}): "
                  f"AP={map_per_class[cls_id]:.4f}, "
                  f"AR@100={mar_100_per_class[cls_id]:.4f}")

    print("=================================\n")

    return map_results


def match_predictions_maskrcnn(predictions, targets, iou_threshold=0.5):

    y_true_all = []
    y_scores_all = []
    y_pred_labels_all = []

    for preds, gts in zip(predictions, targets):
        pred_boxes = preds['boxes'].cpu()
        pred_labels = preds['labels'].cpu()
        pred_scores = preds['scores'].cpu()

        gt_boxes = gts['boxes'].cpu()
        gt_labels = gts['labels'].cpu()

        if len(gt_boxes) == 0:

            y_true_all.extend([0] * len(pred_boxes))
            y_scores_all.extend(pred_scores.tolist())
            y_pred_labels_all.extend(pred_labels.tolist())
            continue


        iou_matrix = box_iou(pred_boxes, gt_boxes)


        for i, (box, label, score) in enumerate(zip(pred_boxes, pred_labels, pred_scores)):
            max_iou, gt_idx = iou_matrix[i].max(0)
            if max_iou >= iou_threshold and label == gt_labels[gt_idx]:
                y_true_all.append(1)
            else:
                y_true_all.append(0)
            y_scores_all.append(score.item())
            y_pred_labels_all.append(label.item())

    return np.array(y_true_all), np.array(y_scores_all), np.array(y_pred_labels_all)


# =============================================================================
# FUNCTIONS FOR OPTUNA STUDUIES AND OPTUNA STUDY ANALYSIS
# =============================================================================    

def get_model_for_tuning(model_tune,fg_iou_thresh, rpn_positive_iou_thresh, rpn_negative_iou_thresh, fpn_grad_flow=True):


    best_anchor_sizes = ((31,), (73,), (131,), (210,), (330,))
    best_aspect_ratios = ((0.5, 1.2, 1.7),) * len(best_anchor_sizes)
    anchor_generator = AnchorGenerator(sizes=best_anchor_sizes, aspect_ratios=best_aspect_ratios)

    model_tune.rpn.positive_iou_thresh = rpn_positive_iou_thresh
    model_tune.rpn.negative_iou_thresh = rpn_negative_iou_thresh
    model_tune.rpn.anchor_generator = anchor_generator

    model_tune.roi_heads.fg_iou_thresh = fg_iou_thresh
    model_tune.roi_heads.bg_iou_thresh = fg_iou_thresh * 0.5
    
    if fpn_grad_flow:
        
        for param in model_tune.backbone.fpn.parameters():
            param.requires_grad = True
    else:
        for param in model_tune.backbone.fpn.parameters():
            param.requires_grad = False

    return model_tune
    
    


def analyze_optuna_study(study):
    print("=" * 60)
    print("OPTUNA STUDY ANALYTICS - HYPERPARAMETER OPTIMIZATION RESULTS")
    print("=" * 60)

    print(f"\nðŸ“Š STUDY OVERVIEW:")
    print(f"   Study Name: {study.study_name}")
    print(f"   Direction: {study.direction.name}")
    print(f"   Total Trials: {len(study.trials)}")
    print(f"   Completed Trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
    print(f"   Pruned Trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}")
    print(f"   Failed Trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])}")

    if study.best_trial:
        print(f"\nðŸ† BEST TRIAL (Trial #{study.best_trial.number}):")
        print(f"   Best Value (mAP): {study.best_trial.value:.4f}")
        print(f"   Best Parameters:")
        for key, value in study.best_trial.params.items():
            if isinstance(value, float):
                print(f"      {key}: {value:.6f}")
            else:
                print(f"      {key}: {value}")

    df = study.trials_dataframe()
    completed_trials = df[df['state'] == 'COMPLETE']

    if len(completed_trials) > 0:
        print(f"\nðŸ“ˆ PERFORMANCE STATISTICS:")
        print(f"   Mean mAP: {completed_trials['value'].mean():.4f}")
        print(f"   Std mAP: {completed_trials['value'].std():.4f}")
        print(f"   Best mAP: {completed_trials['value'].max():.4f}")
        print(f"   Worst mAP: {completed_trials['value'].min():.4f}")
        print(f"   Median mAP: {completed_trials['value'].median():.4f}")

        print(f"\nðŸ”¥ TOP 5 TRIALS:")
        top_5 = completed_trials.nlargest(5, 'value')
        for idx, (_, row) in enumerate(top_5.iterrows(), 1):
            print(f"   #{idx} - Trial {int(row['number'])}: mAP = {row['value']:.4f}")

        param_columns = [col for col in df.columns if col.startswith('params_')]

        if param_columns:
            print(f"\nðŸ”§ HYPERPARAMETER RANGES:")
            for param_col in param_columns:
                param_name = param_col.replace('params_', '')
                param_values = completed_trials[param_col].dropna()

                if len(param_values) > 0:
                    if param_values.dtype in ['int64', 'float64']:
                        print(f"   {param_name}:")
                        print(f"      Range: [{param_values.min():.6f}, {param_values.max():.6f}]")
                        print(f"      Mean: {param_values.mean():.6f}")
                    else:
                        unique_vals = param_values.unique()
                        print(f"   {param_name}: {list(unique_vals)}")

    plt.style.use('default')
    fig = plt.figure(figsize=(20, 15))

    if len(completed_trials) > 1:
        plt.subplot(2, 3, 1)
        plt.plot(completed_trials['number'], completed_trials['value'], 'b-o', markersize=4)
        plt.axhline(y=completed_trials['value'].max(), color='r', linestyle='--', alpha=0.7,
                    label=f'Best: {completed_trials["value"].max():.4f}')
        plt.xlabel('Trial Number')
        plt.ylabel('mAP Score')
        plt.title('Optimization History')
        plt.grid(True, alpha=0.3)
        plt.legend()

        plt.subplot(2, 3, 2)
        plt.hist(completed_trials['value'], bins=min(20, len(completed_trials)//2),
                 alpha=0.7, color='skyblue', edgecolor='black')
        plt.axvline(completed_trials['value'].mean(), color='red', linestyle='--',
                    label=f'Mean: {completed_trials["value"].mean():.4f}')
        plt.xlabel('mAP Score')
        plt.ylabel('Frequency')
        plt.title('Score Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)

        plt.subplot(2, 3, 3)
        state_counts = df['state'].value_counts()
        colors = ['lightgreen', 'lightcoral', 'lightyellow', 'lightblue']
        plt.pie(state_counts.values, labels=state_counts.index, autopct='%1.1f%%',
                colors=colors[:len(state_counts)])
        plt.title('Trial States Distribution')

        try:
            if len(param_columns) > 1:
                plt.subplot(2, 3, 4)
                importances = optuna.importance.get_param_importances(study)

                if importances:
                    params = list(importances.keys())
                    values = list(importances.values())

                    plt.barh(params, values, color='lightsteelblue')
                    plt.xlabel('Importance')
                    plt.title('Parameter Importance')
                    plt.grid(True, alpha=0.3)
        except:
            plt.subplot(2, 3, 4)
            plt.text(0.5, 0.5, 'Parameter Importance\nNot Available',
                     ha='center', va='center', fontsize=12)
            plt.xlim(0, 1)
            plt.ylim(0, 1)
            plt.title('Parameter Importance')

        plt.subplot(2, 3, 5)
        running_best = completed_trials['value'].cummax()
        plt.plot(completed_trials['number'], running_best, 'g-', linewidth=2, label='Running Best')
        plt.scatter(completed_trials['number'], completed_trials['value'],
                    alpha=0.6, s=20, c='blue', label='Trial Scores')
        plt.xlabel('Trial Number')
        plt.ylabel('mAP Score')
        plt.title('Running Best Score')
        plt.legend()
        plt.grid(True, alpha=0.3)

        numeric_params = []
        for param_col in param_columns:
            if completed_trials[param_col].dropna().dtype in ['int64', 'float64']:
                numeric_params.append(param_col)

        if len(numeric_params) > 1:
            plt.subplot(2, 3, 6)
            correlation_data = completed_trials[numeric_params + ['value']].corr()
            sns.heatmap(correlation_data, annot=True, cmap='coolwarm', center=0,
                        square=True, cbar_kws={'label': 'Correlation'})
            plt.title('Parameter-Performance Correlation')
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
        else:
            plt.subplot(2, 3, 6)
            plt.text(0.5, 0.5, 'Correlation Matrix\nNot Enough Numeric\nParameters',
                     ha='center', va='center', fontsize=12)
            plt.xlim(0, 1)
            plt.ylim(0, 1)
            plt.title('Parameter Correlation')

        plt.tight_layout()
        plt.show()

        print(f"\nðŸ“‹ DETAILED TRIAL STATISTICS:")
        print("=" * 80)

        summary_stats = completed_trials['value'].describe()
        print(f"Count:     {summary_stats['count']:.0f}")
        print(f"Mean:      {summary_stats['mean']:.6f}")
        print(f"Std:       {summary_stats['std']:.6f}")
        print(f"Min:       {summary_stats['min']:.6f}")
        print(f"25%:       {summary_stats['25%']:.6f}")
        print(f"50%:       {summary_stats['50%']:.6f}")
        print(f"75%:       {summary_stats['75%']:.6f}")
        print(f"Max:       {summary_stats['max']:.6f}")

        print(f"\nðŸ’¡ INSIGHTS:")
        improvement = completed_trials['value'].max() - completed_trials['value'].min()
        print(f"   â€¢ Total improvement: {improvement:.4f} mAP points")

        if len(completed_trials) > 1:
            convergence_window = min(10, len(completed_trials) // 3)
            recent_best = completed_trials.tail(convergence_window)['value'].max()
            early_best = completed_trials.head(convergence_window)['value'].max()
            if recent_best > early_best:
                print(f"   â€¢ Study is still improving (recent trials performing better)")
            else:
                print(f"   â€¢ Study may be converging (recent performance plateau)")
    else:
        print("\nâŒ No completed trials found for analysis!")

    print(f"\n" + "=" * 60)
    print("ANALYSIS COMPLETE")
    print("=" * 60)



def plot_learning_curves(checkpoint_path, name="Learning Curve", legacy_checkpoint=False):
    # Load checkpoint
    checkpoint = torch.load(checkpoint_path, map_location="cpu")

    # Handle legacy vs new checkpoints
    if legacy_checkpoint:
        train_losses = [1.8902, 1.6821, 1.5692, 1.4875]
        val_losses = [1.7815, 1.5845, 1.4906, 1.4360]
        val_maps = None
    else:
        train_losses = checkpoint["train_losses"]
        val_losses = checkpoint["val_losses"]
        val_maps = checkpoint["val_maps"]

    epochs = list(range(1, len(train_losses) + 1))  # ensure list of ints

    fig, ax1 = plt.subplots(figsize=(8, 5))

    # Plot losses
    ax1.plot(epochs, train_losses, label="Train Loss", color="blue")
    ax1.plot(epochs, val_losses, label="Val Loss", color="orange")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")
    ax1.grid(True, linestyle="--", alpha=0.6)

    # Force integer x-ticks
    ax1.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))

    # Plot val mAP if not legacy
    if val_maps is not None:
        ax2 = ax1.twinx()
        ax2.plot(epochs, val_maps, label="Val mAP", color="green", linestyle="--")
        ax2.set_ylabel("Validation mAP")
        ax2.legend(loc="lower right")
        ax2.grid(False)

    plt.title(name)
    plt.tight_layout()
    plt.show()