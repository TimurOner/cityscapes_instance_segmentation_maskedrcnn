# -*- coding: utf-8 -*-
"""data_funs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOibGbD61EdyEF2e7AK7dyickJQtr9F7
"""





# Standard Library
import sys,re,os
import random
import cv2
import numpy as np
import pandas as pd
import gc
import copy
from collections import namedtuple, Counter, defaultdict
import warnings,json
from IPython.display import clear_output

# Data Science & Visualization
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sns

# PyTorch and Torchvision
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, MultiStepLR
import torchvision
from PIL import Image
from torchvision import transforms
from torchvision import transforms as T
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

# Torchvision Detection Models and Utils (unused in the provided code)
from torchvision.models.detection import MaskRCNN
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.backbone_utils import BackboneWithFPN
import torchvision.models.detection.mask_rcnn as mask_rcnn
import torchvision.models.detection.faster_rcnn as faster_rcnn
from torchvision.models.detection import maskrcnn_resnet50_fpn
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor
from torchvision.models.detection.anchor_utils import AnchorGenerator
from torchvision.ops import box_convert,box_iou, generalized_box_iou
import torchvision.ops.boxes as box_ops

# Safetensor data loading tools
from safetensors.torch import load_file

#Torchmetrics
from torchmetrics.detection import MeanAveragePrecision
from torchmetrics.detection.mean_ap import MeanAveragePrecision

#Scipy and sklearn
from scipy.optimize import linear_sum_assignment
from sklearn.cluster import KMeans

#Mixed precision training
from torch.cuda.amp import autocast, GradScaler

# Progress bar
from tqdm import tqdm

# Set device


from pycocotools import mask as maskUtils
from pycocotools import mask as coco_mask

#Wandb
import wandb


# =============================================================================
# LABELS AND ANNOTATION
# =============================================================================


Label = namedtuple('Label', [
    'name', 'id', 'trainId', 'category', 'categoryId', 'hasInstances', 'ignoreInEval', 'color'
])


cityscapes_labels = [
    Label('unlabeled',            0,   255, 'void',         0, False, True,  (0,   0,   0)),
    Label('ego vehicle',          1,   255, 'void',         0, False, True,  (0,   0,   0)),
    Label('rectification border', 2,   255, 'void',         0, False, True,  (0,   0,   0)),
    Label('out of roi',           3,   255, 'void',         0, False, True,  (0,   0,   0)),
    Label('static',               4,   255, 'void',         0, False, True,  (0,   0,   0)),
    Label('dynamic',              5,   255, 'void',         0, False, True,  (111, 74,  0)),
    Label('ground',               6,   255, 'void',         0, False, True,  (81,  0,  81)),
    Label('road',                 7,   0,   'flat',         1, False, False, (128, 64, 128)),
    Label('sidewalk',             8,   1,   'flat',         1, False, False, (244, 35, 232)),
    Label('parking',              9,   255, 'flat',         1, False, True,  (250, 170, 160)),
    Label('rail track',           10,  255, 'flat',         1, False, True,  (230, 150, 140)),
    Label('building',             11,  2,   'construction', 2, False, False, (70,  70,  70)),
    Label('wall',                 12,  3,   'construction', 2, False, False, (102, 102, 156)),
    Label('fence',                13,  4,   'construction', 2, False, False, (190, 153, 153)),
    Label('guard rail',           14,  255, 'construction', 2, False, True,  (180, 165, 180)),
    Label('bridge',               15,  255, 'construction', 2, False, True,  (150, 100, 100)),
    Label('tunnel',               16,  255, 'construction', 2, False, True,  (150, 120, 90)),
    Label('pole',                 17,  5,   'object',       3, False, False, (153, 153, 153)),
    Label('polegroup',            18,  255, 'object',       3, False, True,  (153, 153, 153)),
    Label('traffic light',        19,  6,   'object',       3, False, False, (250, 170, 30)),
    Label('traffic sign',         20,  7,   'object',       3, False, False, (220, 220, 0)),
    Label('vegetation',           21,  8,   'nature',       4, False, False, (107, 142, 35)),
    Label('terrain',              22,  9,   'nature',       4, False, False, (152, 251, 152)),
    Label('sky',                  23,  10,  'sky',          5, False, False, (70,  130, 180)),
    Label('person',               24,  11,  'human',        6, True,  False, (220, 20,  60)),
    Label('rider',                25,  12,  'human',        6, True,  False, (255, 0,   0)),
    Label('car',                  26,  13,  'vehicle',      7, True,  False, (0,   0,   142)),
    Label('truck',                27,  14,  'vehicle',      7, True,  False, (0,   0,   70)),
    Label('bus',                  28,  15,  'vehicle',      7, True,  False, (0,   60,  100)),
    Label('caravan',              29,  255, 'vehicle',      7, True,  True,  (0,   0,   90)),
    Label('trailer',              30,  255, 'vehicle',      7, True,  True,  (0,   0,   110)),
    Label('train',                31,  16,  'vehicle',      7, True,  False, (0,   80,  100)),
    Label('motorcycle',           32,  17,  'vehicle',      7, True,  False, (0,   0,   230)),
    Label('bicycle',              33,  18,  'vehicle',      7, True,  False, (119, 11,  32)),
    Label('license plate',        -1,  -1,  'vehicle',      7, False, True,  (0,   0,   142)),
]

trainId_to_name = {
        1: 'road', 2: 'sidewalk', 3: 'building', 4: 'wall', 5: 'fence',
        6: 'pole', 7: 'traffic light', 8: 'traffic sign', 9: 'vegetation',
        10: 'terrain', 11: 'sky', 12: 'person', 13: 'rider', 14: 'car',
        15: 'truck', 16: 'bus', 17: 'train', 18: 'motorcycle', 19: 'bicycle'
    }


# =============================================================================
# FILENAME PARSING UTILITIES AND DATA LOADING
# =============================================================================
def load_partition_data(partition_num, base_path,
                        img_prefix='train_images_partition_',
                        ann_prefix='instance_ann_partition_'):
    """
    Loads a single partition's images and annotations from safetensors files.
    """
    img_file = f"{img_prefix}{partition_num}.safetensors"
    ann_file = f"{ann_prefix}{partition_num}.safetensors"

    img_path = os.path.join(base_path, img_file)
    ann_path = os.path.join(base_path, ann_file)

    images_tensor = load_file(img_path)['images']
    annotations_tensor = load_file(ann_path)['instance_annotations']

    return images_tensor, annotations_tensor


def collate_fn(batch):
    return tuple(zip(*batch))


def load_data_loaders_and_datasets(split="train",
                      root_path="/content/drive/MyDrive/Vision Project/partitioned_data",
                      partitions=8, batch_size=4, num_workers=0):
    """
    Loads all partitions for a given split ('train' or 'val') into DataLoaders.
    """
    # Base path for split
    base_path = os.path.join(root_path, split)

    # Prefixes depend on split
    img_prefix = f"{split}_images_partition_"
    ann_prefix = f"instance_ann_partition_"

    datasets = []
    loaders = []

    for partition_num in range(1, partitions + 1):
        images_tensor, annotations_tensor = load_partition_data(
            partition_num, base_path, img_prefix, ann_prefix
        )

        dataset = MaskRCNNDatasetFromTensors(images_tensor, annotations_tensor)
        datasets.append(dataset)

        print(f"[{split.capitalize()}] Partition {partition_num} - "
              f"Images: {images_tensor.shape}, Annotations: {annotations_tensor.shape}")

    for i, dataset in enumerate(datasets, start=1):
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=(split=="train"),
                            num_workers=num_workers, collate_fn=collate_fn)
        loaders.append(loader)
        print(f"[{split.capitalize()} Loader] Partition {i} - {len(dataset)} samples")

    return loaders,datasets
    
def load_data_loader_and_dataset_for_partition(
    partition_num,
    split="train",
    root_path="/content/drive/MyDrive/Vision Project/partitioned_data",
    batch_size=4,
    num_workers=0
):
    """
    Loads a single partition's dataset and DataLoader for a given split ('train' or 'val').
    """
    base_path = os.path.join(root_path, split)
    img_prefix = f"{split}_images_partition_"
    ann_prefix = f"instance_ann_partition_"

    images_tensor, annotations_tensor = load_partition_data(
        partition_num, base_path, img_prefix, ann_prefix
    )

    dataset = MaskRCNNDatasetFromTensors(images_tensor, annotations_tensor)

    print(f"[{split.capitalize()}] Partition {partition_num} - "
          f"Images: {images_tensor.shape}, Annotations: {annotations_tensor.shape}")

    loader = DataLoader(
        dataset, batch_size=batch_size,
        shuffle=(split == "train"),
        num_workers=num_workers,
        collate_fn=collate_fn
    )

    print(f"[{split.capitalize()} Loader] Partition {partition_num} - {len(dataset)} samples")

    return loader, dataset

def load_partition_tensors(partition_num, base_path):
    img_path = f"{base_path}/train_images_partition_{partition_num}.safetensors"
    ann_path = f"{base_path}/instance_ann_partition_{partition_num}.safetensors"
    images_tensor = load_file(img_path)['images']
    annotations_tensor = load_file(ann_path)['instance_annotations']
    return images_tensor, annotations_tensor




# =============================================================================
# DATA INTEGRITY CHECKS AND ANALYZERS
# =============================================================================

def visualize_random_sample_with_masks(images_tensor, annotations_tensor):
   
    idx = random.randint(0, images_tensor.shape[0] - 1)

    img = images_tensor[idx].cpu().permute(1, 2, 0).numpy()
    img = (img - img.min()) / (img.max() - img.min())

    ann_map = annotations_tensor[idx].cpu().numpy().astype(np.int32)
    instance_ids = np.unique(ann_map)
    instance_ids = instance_ids[instance_ids != 0]

    plt.figure(figsize=(12, 8))
    plt.imshow(img)

    for inst_id in instance_ids:
        mask = ann_map == inst_id
        color_mask = np.zeros((*mask.shape, 4))
        color_mask[mask, :3] = np.random.rand(3)
        color_mask[mask, 3] = 0.4
        plt.imshow(color_mask)

    plt.title(f"Random Sample {idx} - {len(instance_ids)} instances")
    plt.axis('off')
    plt.show()

trainId_to_name = {
        1: 'road', 2: 'sidewalk', 3: 'building', 4: 'wall', 5: 'fence',
        6: 'pole', 7: 'traffic light', 8: 'traffic sign', 9: 'vegetation',
        10: 'terrain', 11: 'sky', 12: 'person', 13: 'rider', 14: 'car',
        15: 'truck', 16: 'bus', 17: 'train', 18: 'motorcycle', 19: 'bicycle'
    }
    
def analyze_dataset(dataset, small_thresh=32*32, medium_thresh=96*96):
   
    # Map from class id to class name (adjust based on your dataset)
    trainId_to_name = {
        1: 'road', 2: 'sidewalk', 3: 'building', 4: 'wall', 5: 'fence',
        6: 'pole', 7: 'traffic light', 8: 'traffic sign', 9: 'vegetation',
        10: 'terrain', 11: 'sky', 12: 'person', 13: 'rider', 14: 'car',
        15: 'truck', 16: 'bus', 17: 'train', 18: 'motorcycle', 19: 'bicycle'
    }

    class_counts = Counter()
    size_counts = {'small': 0, 'medium': 0, 'large': 0}
    class_size_dist = defaultdict(lambda: {'small': 0, 'medium': 0, 'large': 0})

    areas = []
    widths = []
    heights = []
    aspect_ratios = []

    print("Analyzing dataset...")

    for i, (_, target) in enumerate(dataset):
        try:
            for box, label, area in zip(target['boxes'], target['labels'], target['area']):
                label = label.item()
                area_val = area.item()
                width = (box[2] - box[0]).item()
                height = (box[3] - box[1]).item()
                aspect_ratio = width / height if height > 0 else 0

                # Update statistics
                class_counts[label] += 1
                areas.append(area_val)
                widths.append(width)
                heights.append(height)
                aspect_ratios.append(aspect_ratio)

                # Size categorization
                if area_val <= small_thresh:
                    size_counts['small'] += 1
                    class_size_dist[label]['small'] += 1
                elif area_val <= medium_thresh:
                    size_counts['medium'] += 1
                    class_size_dist[label]['medium'] += 1
                else:
                    size_counts['large'] += 1
                    class_size_dist[label]['large'] += 1

        except Exception as e:
            print(f"Skipping image {i}: {e}")
            continue

    # Convert to numpy arrays for statistics
    areas = np.array(areas)
    widths = np.array(widths)
    heights = np.array(heights)
    aspect_ratios = np.array(aspect_ratios)

    total_objects = sum(class_counts.values())

    print(f"\n=== DATASET ANALYSIS ===")
    print(f"Total objects: {total_objects}")
    print(f"Total images processed: {len(dataset)}")
    print(f"Average objects per image: {total_objects / len(dataset):.2f}")

    print(f"\n=== SIZE DISTRIBUTION ===")
    print(f"Small objects (≤{int(np.sqrt(small_thresh))}x{int(np.sqrt(small_thresh))}px): {size_counts['small']} ({size_counts['small']/total_objects*100:.1f}%)")
    print(f"Medium objects ({int(np.sqrt(small_thresh))}-{int(np.sqrt(medium_thresh))}px): {size_counts['medium']} ({size_counts['medium']/total_objects*100:.1f}%)")
    print(f"Large objects (>{int(np.sqrt(medium_thresh))}x{int(np.sqrt(medium_thresh))}px): {size_counts['large']} ({size_counts['large']/total_objects*100:.1f}%)")

    print(f"\n=== OBJECT DIMENSIONS ===")
    print(f"Area stats - Mean: {areas.mean():.1f}, Median: {np.median(areas):.1f}, Std: {areas.std():.1f}")
    print(f"Width stats - Mean: {widths.mean():.1f}, Median: {np.median(widths):.1f}, Min: {widths.min():.1f}, Max: {widths.max():.1f}")
    print(f"Height stats - Mean: {heights.mean():.1f}, Median: {np.median(heights):.1f}, Min: {heights.min():.1f}, Max: {heights.max():.1f}")
    print(f"Aspect ratio stats - Mean: {aspect_ratios.mean():.2f}, Median: {np.median(aspect_ratios):.2f}, Min: {aspect_ratios.min():.2f}, Max: {aspect_ratios.max():.2f}")

    print(f"\n=== CLASS DISTRIBUTION ===")
    for class_id, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True):
        class_name = trainId_to_name.get(class_id, f"class_{class_id}")
        print(f"{class_name:<15}: {count:>6} ({count/total_objects*100:>5.1f}%)")

    print(f"\n=== SMALL OBJECT ANALYSIS ===")
    if size_counts['small'] > 0:
        mask_small = areas <= small_thresh
        small_widths = widths[mask_small]
        small_heights = heights[mask_small]
        small_areas = areas[mask_small]

        print(f"Small object dimensions:")
        print(f"  Width: {small_widths.mean():.1f}±{small_widths.std():.1f} (min: {small_widths.min():.1f}, max: {small_widths.max():.1f})")
        print(f"  Height: {small_heights.mean():.1f}±{small_heights.std():.1f} (min: {small_heights.min():.1f}, max: {small_heights.max():.1f})")
        print(f"  Area: {small_areas.mean():.1f}±{small_areas.std():.1f}")

        print(f"\nSmall objects per class:")
        for class_id, count in sorted(class_counts.items(), key=lambda x: class_size_dist[x[0]]['small'], reverse=True):
            small_count = class_size_dist[class_id]['small']
            if small_count > 0:
                class_name = trainId_to_name.get(class_id, f"class_{class_id}")
                print(f"  {class_name:<15}: {small_count:>4}/{count:>4} ({small_count/count*100:>5.1f}%)")
    else:
        print("No small objects found!")

    return {
        'class_counts': class_counts,
        'size_counts': size_counts,
        'areas': areas,
        'widths': widths,
        'heights': heights,
        'aspect_ratios': aspect_ratios,
        'total_objects': total_objects,
        'class_size_dist': class_size_dist
    }

# =============================================================================
# LABEL MAPPING UTILITIES
# =============================================================================

# Convert label IDs to training IDs
def id_to_trainId(id_num):
    return id_to_trainId_map.get(id_num, None)

# =============================================================================
# DATASET CLASSES
# =============================================================================
        
class MaskRCNNDatasetFromTensors(Dataset):
    def __init__(self, images_tensor, annotations_tensor, id_to_trainId=None):
        assert images_tensor.shape[0] == annotations_tensor.shape[0], "Images and annotations must have the same number of samples"
        self.images = images_tensor
        self.annotations = annotations_tensor.squeeze(1)  # [N, H, W]

        if id_to_trainId is None:
            self.id_to_trainId = {
                7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,
                19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11,
                25: 12, 26: 13, 27: 14, 28: 15, 31: 16,
                32: 17, 33: 18
            }
        else:
            self.id_to_trainId = id_to_trainId

    def __len__(self):
        return self.images.shape[0]

    def __getitem__(self, idx):
        image_tensor = self.images[idx]  # [3, H, W]

        # Reverse normalization: multiply by 255 and round to get original labels
        annotation_float = self.annotations[idx]  # [H, W], float tensor normalized 0-1
        annotation_recovered = (annotation_float * 255).round().to(torch.int64).cpu().numpy()

        masks = []
        boxes = []
        labels = []

        for instance_id in np.unique(annotation_recovered):
            if instance_id < 1000:
                continue

            label_id = instance_id // 1000
            if label_id not in self.id_to_trainId:
                continue

            train_id = self.id_to_trainId[label_id]
            binary_mask = (annotation_recovered == instance_id).astype(np.uint8)

            if binary_mask.sum() == 0:
                continue

            pos = np.where(binary_mask)
            xmin, xmax = np.min(pos[1]), np.max(pos[1])
            ymin, ymax = np.min(pos[0]), np.max(pos[0])

            if xmax <= xmin or ymax <= ymin:
                continue

            boxes.append([xmin, ymin, xmax, ymax])
            masks.append(binary_mask)
            labels.append(train_id + 1)  # +1 for background=0

        if len(boxes) == 0:
            boxes_tensor = torch.zeros((0, 4), dtype=torch.float32)
            labels_tensor = torch.zeros((0,), dtype=torch.int64)
            masks_tensor = torch.zeros((0, annotation_recovered.shape[0], annotation_recovered.shape[1]), dtype=torch.uint8)
        else:
            boxes_tensor = torch.tensor(boxes, dtype=torch.float32)
            labels_tensor = torch.tensor(labels, dtype=torch.int64)
            masks_tensor = torch.tensor(np.stack(masks), dtype=torch.uint8)

        target = {
            "boxes": boxes_tensor,
            "labels": labels_tensor,
            "masks": masks_tensor,
            "image_id": torch.tensor([idx]),
            "area": (boxes_tensor[:, 3] - boxes_tensor[:, 1]) * (boxes_tensor[:, 2] - boxes_tensor[:, 0]) if boxes_tensor.shape[0] > 0 else torch.tensor([]),
            "iscrowd": torch.zeros((labels_tensor.shape[0],), dtype=torch.int64)
        }

        return image_tensor, target
        
        

# =============================================================================
# DATASET VALIDATION AND ANALYSIS UTILITIES
# =============================================================================

# Alternative function to just inspect your checkpoint
def inspect_checkpoint(checkpoint_path):
    """Inspect checkpoint structure without loading model"""
    print(f"ðŸ” Inspecting checkpoint: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    print(f"ðŸ“‚ Checkpoint type: {type(checkpoint)}")
    print(f"ðŸ“‚ Top-level keys: {list(checkpoint.keys()) if isinstance(checkpoint, dict) else 'Not a dict'}")

    # Try to find the actual state dict
    state_dict = None
    if isinstance(checkpoint, dict):
        for key in ['model_state_dict', 'state_dict', 'model']:
            if key in checkpoint:
                state_dict = checkpoint[key]
                print(f"âœ… Found state dict under key: '{key}'")
                break
        if state_dict is None:
            state_dict = checkpoint
            print("âœ… Using checkpoint as state dict")
    else:
        state_dict = checkpoint
        print("âœ… Checkpoint is directly a state dict")

    if isinstance(state_dict, dict):
        print(f"ðŸ“Š State dict has {len(state_dict)} parameters")
        print(f"ðŸ” Parameter keys (first 10):")
        for i, key in enumerate(list(state_dict.keys())[:10]):
            tensor_shape = state_dict[key].shape if hasattr(state_dict[key], 'shape') else 'No shape'
            print(f"  {i+1}. {key}: {tensor_shape}")

        # Check for common components
        components = ['backbone', 'rpn', 'roi_heads', 'classifier', 'box_predictor', 'mask_predictor']
        found_components = []
        for comp in components:
            if any(comp in key for key in state_dict.keys()):
                found_components.append(comp)
        print(f"ðŸŽ¯ Found components: {found_components}")

    return checkpoint

    
def run_inference(model, dataset, idx, device='cuda'):
    model.eval()
    model.to(device)

    # Get one image and move to device
    image_tensor, _ = dataset[idx]
    image_tensor = image_tensor.to(device)

    # Remove batch dimension if present - model expects [C, H, W]
    if image_tensor.dim() == 4:
        image_tensor = image_tensor.squeeze(0)

    with torch.no_grad():
        # Pass as list of 3D tensors
        prediction = model([image_tensor])

    return prediction[0], image_tensor.cpu()


def visualize_prediction(image_tensor, prediction, score_threshold=0.5):
    image = image_tensor.permute(1, 2, 0).numpy()
    masks = prediction['masks'] > 0.5
    boxes = prediction['boxes']
    labels = prediction['labels']
    scores = prediction['scores']

    plt.figure(figsize=(10, 10))
    plt.imshow(image)
    ax = plt.gca()

    for i in range(len(masks)):
        if scores[i] < score_threshold:
            continue

        mask = masks[i][0].cpu().numpy()
        color = np.random.rand(3)
        masked = np.ma.masked_where(mask == 0, mask)
        ax.imshow(masked, alpha=0.5, cmap='viridis')

        box = boxes[i].cpu().numpy()
        label_id = labels[i].item()
        label_name = trainId_to_name.get(label_id, "unknown")

        ax.add_patch(plt.Rectangle(
            (box[0], box[1]), box[2]-box[0], box[3]-box[1],
            fill=False, edgecolor='red', linewidth=2
        ))
        ax.text(
            box[0], box[1] - 5,
            f"{label_id}: {label_name}",
            color='white', fontsize=10, backgroundcolor='red'
        )

    plt.axis('off')
    plt.show()
    
def visualize_ground_truth(dataset, idx):
    image_tensor, target = dataset[idx]

    # Convert image to HWC format for plotting
    image = image_tensor.permute(1, 2, 0).numpy()

    boxes = target['boxes']
    labels = target['labels']
    masks = target['masks']

    plt.figure(figsize=(10, 10))
    plt.imshow(image)
    ax = plt.gca()

    for i in range(len(masks)):
        mask = masks[i].numpy()
        color = np.random.rand(3)
        masked = np.ma.masked_where(mask == 0, mask)
        ax.imshow(masked, alpha=0.5, cmap='gray')

        box = boxes[i].numpy()
        label = labels[i].item()

        ax.add_patch(plt.Rectangle(
            (box[0], box[1]), box[2]-box[0], box[3]-box[1],
            fill=False, edgecolor='lime', linewidth=2
        ))

        ax.text(box[0], box[1] - 5, f"GT Label: {label}",
                color='white', fontsize=10, backgroundcolor='green')

    plt.axis('off')
    plt.title(f"Ground Truth for Sample {idx}")
    plt.show()
    




def match_predictions_maskrcnn(predictions, targets, iou_threshold=0.5):

    y_true_all = []
    y_scores_all = []
    y_pred_labels_all = []

    for preds, gts in zip(predictions, targets):
        pred_boxes = preds['boxes'].cpu()
        pred_labels = preds['labels'].cpu()
        pred_scores = preds['scores'].cpu()

        gt_boxes = gts['boxes'].cpu()
        gt_labels = gts['labels'].cpu()

        if len(gt_boxes) == 0:

            y_true_all.extend([0] * len(pred_boxes))
            y_scores_all.extend(pred_scores.tolist())
            y_pred_labels_all.extend(pred_labels.tolist())
            continue


        iou_matrix = box_iou(pred_boxes, gt_boxes)


        for i, (box, label, score) in enumerate(zip(pred_boxes, pred_labels, pred_scores)):
            max_iou, gt_idx = iou_matrix[i].max(0)
            if max_iou >= iou_threshold and label == gt_labels[gt_idx]:
                y_true_all.append(1)
            else:
                y_true_all.append(0)
            y_scores_all.append(score.item())
            y_pred_labels_all.append(label.item())

    return np.array(y_true_all), np.array(y_scores_all), np.array(y_pred_labels_all)


def visualize_random_sample_with_masks_from_dataset(dataset):
    idx = random.randint(0, len(dataset) - 1)
    img_tensor, target = dataset[idx]


    # Image tensor: [3, H, W] -> [H, W, 3]
    img = img_tensor.cpu().permute(1, 2, 0).numpy()
    img = (img - img.min()) / (img.max() - img.min())  # Normalize for display

    masks = target['masks'].cpu().numpy()  # [num_instances, H, W]

    num_instances = masks.shape[0]

    plt.figure(figsize=(12, 8))
    plt.imshow(img)

    for i in range(num_instances):
        mask = masks[i]
        color_mask = np.zeros((*mask.shape, 4))
        color_mask[mask == 1, :3] = np.random.rand(3)  # random color
        color_mask[mask == 1, 3] = 0.4  # alpha
        plt.imshow(color_mask)

    plt.title(f"Random Sample {idx} with {num_instances} instances")
    plt.axis('off')
    plt.show()