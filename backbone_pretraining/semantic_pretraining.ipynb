{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2eATwtyft55krRp3qhxXF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rIw-NaT_lgwb"},"outputs":[],"source":["# =========================\n","# Imports\n","# =========================\n","import os\n","from collections import defaultdict\n","\n","import numpy as np\n","import torch\n","import torchvision\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n"]},{"cell_type":"code","source":["# =========================\n","# Dataset Class for Semantic Segmentation\n","# =========================\n","class SegmentationDatasetFromDF(Dataset):\n","    def __init__(self, df_img, df_seg, transform=None, mask_transform=None):\n","        self.df_img = df_img.reset_index(drop=True)\n","        self.df_seg = df_seg.reset_index(drop=True)\n","        self.transform = transform\n","        self.mask_transform = mask_transform\n","\n","        # Vectorized mapping from original mask IDs to training IDs\n","        self.id_to_trainId_vec = np.vectorize(lambda x: id_to_trainId_map.get(x, 255))\n","\n","    def __len__(self):\n","        return len(self.df_img)\n","\n","    def __getitem__(self, idx):\n","        # Load image\n","        image = self.df_img.loc[idx, 'image_data']\n","        if not isinstance(image, Image.Image):\n","            image = Image.fromarray(image)\n","\n","        # Load mask\n","        mask = self.df_seg.loc[idx, 'annotation_data']\n","        if not isinstance(mask, np.ndarray):\n","            mask = np.array(mask)\n","\n","        # Convert mask IDs to train IDs\n","        mask_converted = self.id_to_trainId_vec(mask).astype(np.uint8)\n","        mask_converted = Image.fromarray(mask_converted)\n","\n","        # Apply transforms\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.mask_transform:\n","            mask_converted = self.mask_transform(mask_converted)\n","\n","        return image, mask_converted\n"],"metadata":{"id":"Gg-CS5trlhit"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Training and Validation Functions\n","# =========================\n","def train_one_epoch(model, dataloader, optimizer, criterion, device, epoch_num):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for batch_idx, (images, masks) in enumerate(dataloader):\n","        images, masks = images.to(device), masks.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)['out']\n","        loss = criterion(outputs, masks.squeeze(1).long())\n","\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(dataloader):\n","            print(f\"Epoch [{epoch_num}], Batch [{batch_idx + 1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n","\n","    epoch_loss = running_loss / len(dataloader)\n","    print(f\"Epoch [{epoch_num}] completed. Average Loss: {epoch_loss:.4f}\")\n","    return epoch_loss\n","\n","\n","def validate_one_epoch(model, dataloader, criterion, device, epoch_num):\n","    model.eval()\n","    running_val_loss = 0.0\n","\n","    with torch.no_grad():\n","        for batch_idx, (images, masks) in enumerate(dataloader):\n","            images, masks = images.to(device), masks.to(device)\n","\n","            outputs = model(images)['out']\n","            loss = criterion(outputs, masks.squeeze(1).long())\n","            running_val_loss += loss.item()\n","\n","            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(dataloader):\n","                print(f\"Validation Epoch [{epoch_num}], Batch [{batch_idx + 1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n","\n","    avg_val_loss = running_val_loss / len(dataloader)\n","    print(f\"Validation Epoch [{epoch_num}] completed. Average Loss: {avg_val_loss:.4f}\")\n","    return avg_val_loss\n"],"metadata":{"id":"Egd4s1kFlvpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Model Setup\n","# =========================\n","N_CLASSES = 19\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load pretrained DeepLabV3 with ResNet50 backbone\n","model_backbone_pretrained = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n","\n","# Replace classifier for custom number of classes\n","model_backbone_pretrained.classifier = torchvision.models.segmentation.deeplabv3.DeepLabHead(2048, N_CLASSES)\n","model = model_backbone_pretrained.to(device)\n"],"metadata":{"id":"I2ot5i07lwSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Training Settings\n","# =========================\n","num_epochs = 19\n","save_every_n = 2\n","checkpoint_dir = '/content/drive/MyDrive/Vision Project/backbone_resnet50_pretrained'\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","# =========================\n","# Load Checkpoint\n","# =========================\n","checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_10.pth')\n","checkpoint = torch.load(checkpoint_path, map_location=device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n","\n","# Load checkpoint states\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, mode='min', patience=3, factor=0.1, verbose=True\n",")\n","scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","\n","start_epoch = checkpoint['epoch']\n","train_losses = checkpoint['train_losses']\n","val_losses = checkpoint['val_losses']\n","best_val_loss = checkpoint['best_val_loss']\n"],"metadata":{"id":"1URS5AhXlz3l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def analyze_backbone_percent_difference(checkpoint_path, device=\"cpu\", epsilon=1e-8):\n","    \"\"\"\n","    Compare transferred checkpoint backbone vs torchvision DeepLabV3 ResNet-50 backbone.\n","    Shows differences as percentage relative to DeepLab weights.\n","    \"\"\"\n","    # Load checkpoint\n","    ckpt = torch.load(checkpoint_path, map_location=device)\n","    state_dict = ckpt.get('model_state_dict', ckpt)\n","\n","    # Extract backbone weights\n","    ckpt_backbone = {k: v for k, v in state_dict.items() if \"backbone\" in k}\n","    norm_ckpt = {k.replace(\"backbone.body.\", \"\").replace(\"backbone.\", \"\"): v for k, v in ckpt_backbone.items()}\n","\n","    # Load DeepLab backbone\n","    deeplab = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True).backbone\n","    deeplab_state = deeplab.state_dict()\n","\n","    common = set(norm_ckpt.keys()).intersection(deeplab_state.keys())\n","\n","    group_diffs = defaultdict(list)\n","    sample_diffs = {}\n","    for k in sorted(common):\n","        t1 = norm_ckpt[k].detach().to(device)\n","        t2 = deeplab_state[k].detach().to(device)\n","        diff_percent = torch.mean(torch.abs(t1 - t2) / (torch.abs(t2) + epsilon)) * 100.0\n","\n","        group = k.split('.')[0]\n","        group_diffs[group].append(diff_percent.item())\n","        sample_diffs[k] = diff_percent.item()\n","\n","    # Display grouped results\n","    print(f\"âœ… Common keys: {len(common)}\")\n","    for group, diffs in group_diffs.items():\n","        avg_diff = sum(diffs) / len(diffs)\n","        print(f\"ðŸ“‚ {group:<8} | keys: {len(diffs):<4} | avg % diff: {avg_diff:.2f}%\")\n","\n","    # Sample differences\n","    print(\"\\nðŸ”Ž Sample percent differences (first 10):\")\n","    for k in list(sample_diffs.keys())[:10]:\n","        print(f\"  {k:<40} diff={sample_diffs[k]:.2f}%\")\n","\n","    # Plot bar chart\n","    groups, avgs = zip(*[(g, sum(d)/len(d)) for g, d in group_diffs.items\n"],"metadata":{"id":"395r_iYal6dG"},"execution_count":null,"outputs":[]}]}